---
title: "vcf_autoencoder"
output: html_document
date: "2022-12-02"
---

```{python}
import allel
import tensorflow
import numpy
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
````

```{python}

callset_sim = allel.read_vcf('sim.vcf.gz')
callset_sim['calldata/GT'].shape
# callset_sim['samples']
# callset_sim['variants/ID']

callset_sim_test = allel.read_vcf('sim_test.vcf.gz')
callset_sim_test['calldata/GT'].shape

vcf_CureGN = allel.read_vcf('2022-12-01_15-25-37_vcf_CureGN_12012022_variants.vcf.gz')
dims = vcf_CureGN['calldata/GT'].shape
gt = allel.GenotypeArray(vcf_CureGN['calldata/GT'])
np.count_nonzero(np.count_nonzero(gt.reshape(2090,50898,2)==1,axis=1),axis=1)


x_train = callset_sim['calldata/GT'].reshape(5000,500,2).astype("float32")
x_test = callset_sim_test['calldata/GT'].reshape(1200,500,2).astype("float32")

x_init = vcf_CureGN['calldata/GT'].reshape(dims[1],dims[0],dims[2]).astype("float32")
x_train, x_test = train_test_split(x_init,test_size=0.1, random_state=212)

# input_shape = (500,2,1)
input_shape = (dims[0],dims[2],1)
```

```{python}
# Encoder
x = tensorflow.keras.layers.Input(shape=(input_shape), name="encoder_input")

encoder_conv_layer1 = tensorflow.keras.layers.Conv2D(filters=1, kernel_size=(3, 3), padding="same", strides=1, name="encoder_conv_1")(x)
encoder_norm_layer1 = tensorflow.keras.layers.BatchNormalization(name="encoder_norm_1")(encoder_conv_layer1)
encoder_activ_layer1 = tensorflow.keras.layers.LeakyReLU(name="encoder_leakyrelu_1")(encoder_norm_layer1)

encoder_conv_layer2 = tensorflow.keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding="same", strides=1, name="encoder_conv_2")(encoder_activ_layer1)
encoder_norm_layer2 = tensorflow.keras.layers.BatchNormalization(name="encoder_norm_2")(encoder_conv_layer2)
encoder_activ_layer2 = tensorflow.keras.layers.LeakyReLU(name="encoder_activ_layer_2")(encoder_norm_layer2)

encoder_conv_layer3 = tensorflow.keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding="same", strides=2, name="encoder_conv_3")(encoder_activ_layer2)
encoder_norm_layer3 = tensorflow.keras.layers.BatchNormalization(name="encoder_norm_3")(encoder_conv_layer3)
encoder_activ_layer3 = tensorflow.keras.layers.LeakyReLU(name="encoder_activ_layer_3")(encoder_norm_layer3)

encoder_conv_layer4 = tensorflow.keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding="same", strides=2, name="encoder_conv_4")(encoder_activ_layer3)
encoder_norm_layer4 = tensorflow.keras.layers.BatchNormalization(name="encoder_norm_4")(encoder_conv_layer4)
encoder_activ_layer4 = tensorflow.keras.layers.LeakyReLU(name="encoder_activ_layer_4")(encoder_norm_layer4)

encoder_conv_layer5 = tensorflow.keras.layers.Conv2D(filters=256, kernel_size=(3,3), padding="same", strides=1, name="encoder_conv_5")(encoder_activ_layer4)
encoder_norm_layer5 = tensorflow.keras.layers.BatchNormalization(name="encoder_norm_5")(encoder_conv_layer5)
encoder_activ_layer5 = tensorflow.keras.layers.LeakyReLU(name="encoder_activ_layer_5")(encoder_norm_layer5)

shape_before_flatten = tensorflow.keras.backend.int_shape(encoder_activ_layer5)[1:]
encoder_flatten = tensorflow.keras.layers.Flatten()(encoder_activ_layer5)

latent_space_dim = 2

encoder_mu = tensorflow.keras.layers.Dense(units=latent_space_dim, name="encoder_mu")(encoder_flatten)
encoder_log_variance = tensorflow.keras.layers.Dense(units=latent_space_dim, name="encoder_log_variance")(encoder_flatten)

encoder_mu_log_variance_model = tensorflow.keras.models.Model(x, (encoder_mu, encoder_log_variance), name="encoder_mu_log_variance_model")

def sampling(mu_log_variance):
    mu, log_variance = mu_log_variance
    epsilon = tensorflow.keras.backend.random_normal(shape=tensorflow.keras.backend.shape(mu), mean=0.0, stddev=1.0)
    random_sample = mu + tensorflow.keras.backend.exp(log_variance/2) * epsilon
    return random_sample

encoder_output = tensorflow.keras.layers.Lambda(sampling, name="encoder_output")([encoder_mu, encoder_log_variance])

encoder = tensorflow.keras.models.Model(x, encoder_output, name="encoder_model")

decoder_input = tensorflow.keras.layers.Input(shape=(latent_space_dim), name="decoder_input")
decoder_dense_layer1 = tensorflow.keras.layers.Dense(units=numpy.prod(shape_before_flatten), name="decoder_dense_1")(decoder_input)
decoder_reshape = tensorflow.keras.layers.Reshape(target_shape=shape_before_flatten)(decoder_dense_layer1)

decoder_conv_tran_layer1 = tensorflow.keras.layers.Conv2DTranspose(filters=256, kernel_size=(3, 3), padding="same", strides=1, name="decoder_conv_tran_1")(decoder_reshape)
decoder_norm_layer1 = tensorflow.keras.layers.BatchNormalization(name="decoder_norm_1")(decoder_conv_tran_layer1)
decoder_activ_layer1 = tensorflow.keras.layers.LeakyReLU(name="decoder_leakyrelu_1")(decoder_norm_layer1)

decoder_conv_tran_layer2 = tensorflow.keras.layers.Conv2DTranspose(filters=128, kernel_size=(3, 3), padding="same", strides=2, name="decoder_conv_tran_2")(decoder_activ_layer1)
decoder_norm_layer2 = tensorflow.keras.layers.BatchNormalization(name="decoder_norm_2")(decoder_conv_tran_layer2)
decoder_activ_layer2 = tensorflow.keras.layers.LeakyReLU(name="decoder_leakyrelu_2")(decoder_norm_layer2)

decoder_conv_tran_layer3 = tensorflow.keras.layers.Conv2DTranspose(filters=64, kernel_size=(3, 3), padding="same", strides=(2,1), name="decoder_conv_tran_3")(decoder_activ_layer2)
decoder_norm_layer3 = tensorflow.keras.layers.BatchNormalization(name="decoder_norm_3")(decoder_conv_tran_layer3)
decoder_activ_layer3 = tensorflow.keras.layers.LeakyReLU(name="decoder_leakyrelu_3")(decoder_norm_layer3)

decoder_conv_tran_layer4 = tensorflow.keras.layers.Conv2DTranspose(filters=1, kernel_size=(3, 3), padding="same", strides=1, name="decoder_conv_tran_4")(decoder_activ_layer3)
x = tensorflow.keras.layers.LeakyReLU(name="decoder_output")(decoder_conv_tran_layer4)
decoder_output = tensorflow.keras.layers.Cropping2D(cropping=((1, 0)), data_format=None)(x) # this is the added step


decoder = tensorflow.keras.models.Model(decoder_input, decoder_output, name="decoder_model")
```

```{python}
def loss_func(encoder_mu, encoder_log_variance):
    def vae_reconstruction_loss(y_true, y_predict):
        reconstruction_loss_factor = 1000
        reconstruction_loss = tensorflow.keras.backend.mean(tensorflow.keras.backend.square(y_true-y_predict), axis=[1])
        return reconstruction_loss_factor * reconstruction_loss
    def vae_kl_loss(encoder_mu, encoder_log_variance):
        kl_loss = -0.5 * tensorflow.keras.backend.sum(1.0 + encoder_log_variance - tensorflow.keras.backend.square(encoder_mu) - tensorflow.keras.backend.exp(encoder_log_variance), axis=1)
        return kl_loss
    def vae_kl_loss_metric(y_true, y_predict):
        kl_loss = -0.5 * tensorflow.keras.backend.sum(1.0 + encoder_log_variance - tensorflow.keras.backend.square(encoder_mu) - tensorflow.keras.backend.exp(encoder_log_variance), axis=1)
        return kl_loss
    def vae_loss(y_true, y_predict):
        reconstruction_loss = vae_reconstruction_loss(y_true, y_predict)
        kl_loss = vae_kl_loss(y_true, y_predict)
        loss = reconstruction_loss + kl_loss
        return loss
    return vae_loss

```

```{python}
vae_input = tensorflow.keras.layers.Input(shape=(input_shape), name="VAE_input")
vae_encoder_output = encoder(vae_input)
vae_decoder_output = decoder(vae_encoder_output)
vae = tensorflow.keras.models.Model(vae_input, vae_decoder_output, name="VAE")

vae.compile(optimizer=tensorflow.keras.optimizers.Adam(learning_rate=1e-6), loss=loss_func(encoder_mu, encoder_log_variance))
vae.fit(x_train, x_train, epochs=30, batch_size=32, shuffle=True, validation_data=(x_test, x_test))
encoder.save("VAE_encoder.h5")
decoder.save("VAE_decoder.h5")
vae.save("VAE.h5")
```

```{python}
# encoder = tensorflow.keras.models.load_model("VAE_encoder.h5")
vcf_CureGN = allel.read_vcf('merged_curegn_cuimc.vcf.gz')
dims = vcf_CureGN['calldata/GT'].shape
x_full = vcf_CureGN['calldata/GT'].reshape(5961,106375,2,1)
x_full = numpy.dstack((numpy.count_nonzero(x_full==1,axis=2),numpy.count_nonzero(x_full!=1,axis=2)))
x_full = numpy.pad(x_full, pad_width = ((0,0),(1,0),(0,0)),  mode='constant', constant_values=0).astype("float32")
x_full.shape
vae = tensorflow.keras.models.load_model("merged_0110_weights-improvement-194-15720419.00", custom_objects={"vae_loss":loss_func})
encoder = vae.get_layer("encoder_model")
decoder = tensorflow.keras.models.load_model("VAE_decoder.h5")

encoder.compile()
x_init_predicted = encoder.predict(x_full)
to_save = numpy.hstack((vcf_CureGN['samples'].reshape(5961,1),x_init_predicted))
numpy.savetxt("predictions_01182023.csv",to_save,delimiter=",",fmt='%s')

# x_init_predicted = encoder.predict(x_init, batch_size=32)
plt.figure(figsize=(6, 6))
plt.scatter(x_init_predicted[:, 0], x_init_predicted[:, 1], c=numpy.sum(x_init_predicted,axis=1))
plt.colorbar()
plt.show()

```